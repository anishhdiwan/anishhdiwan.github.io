---
---

@string{aps = {American Physical Society,}}

@inproceedings{diwan2025noise,
    title={Noise-conditioned Energy-based Annealed Rewards (NEAR): A Generative Framework for Imitation Learning from Observation},
    author={Diwan, Anish Abhijit and Urain, Julen and Kober, Jens and Peters, Jan},
    booktitle={International Conference on Learning Representations},
    year={2025},
    website={https://anishhdiwan.github.io/noise-conditioned-energy-based-annealed-rewards/},
    abstract={This paper introduces a new imitation learning framework based on energy-based generative models capable of learning complex, physics-dependent, robot motion policies through state-only expert motion trajectories. Our algorithm, called Noise-conditioned Energy-based Annealed Rewards (NEAR), constructs several perturbed versions of the expert’s motion data distribution and learns smooth, and well-defined representations of the data distribution’s energy function using denoising score matching. We propose to use these learnt energy functions as reward functions to learn imitation policies via reinforcement learning. We also present a strategy to gradually switch between the learnt energy functions, ensuring that the learnt rewards are always well-defined in the manifold of policy-generated samples. We evaluate our algorithm on complex humanoid tasks such as locomotion and martial arts and compare it with state-only adversarial imitation learning algorithms like Adversarial Motion Priors (AMP). Our framework sidesteps the optimisation challenges of adversarial imitation learning techniques and produces results comparable to AMP in several quantitative metrics across multiple imitation settings.},
    selected={true},
    preview={near_spin_kick.gif},
    arxiv={2501.14856}
  }